<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yanghao Li</title>
  
  <meta name="author" content="Yanghao Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="images/favicon.jpg" src="images/favicon.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yanghao Li</name>
              </p>
              <p>I am a Research Engineer at <a href="https://ai.facebook.com/">Facebook AI Research (FAIR)</a>, where I work on computer vision and deep learning. I graduated with B.S. and M.S. degrees in Computer Science from <a href="https://english.pku.edu.cn/">Peking University</a> in 2015 and 2018.
              </p>
              <p style="text-align:center">
                <a href="mailto:lyttonhao@gmail.com">Email</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=-VgS8AIAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://github.com/lyttonhao">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/LiYanghao.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Liyanghao_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h1>Publications</h1>
              <!-- <p>
                I'm interested in computer vision, machine learning, optimization, and image processing. Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images. Representative papers are <span class="highlight">highlighted</span>.
              </p> -->
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr><td><h3>2022</h3></td></tr>
          
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/memvit.png' alt="game" width="180" style="border-style: none">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2201.08383">
                <papertitle>MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition</papertitle>
              </a>
              <br>
              Chao-Yuan Wu*, <strong>Yanghao Li*</strong>, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, Christoph Feichtenhofer*
              <br>
              <em>arXiv</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2201.08383">Paper</a>

              <!-- <p></p>
              <p>We study a Memory-augmented Multiscale Vision Transformer (MeMViT), that has a temporal support 30x longer than existing models with only 4.5 more compute. </p> -->
            </td>
          </tr>


          <tr><td><h3>2021</h3></td></tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/mvit_v2.png' alt="game" width="180" style="border-style: none">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2112.01526">
                <papertitle>Improved Multiscale Vision Transformers for Classification and Detection</papertitle>
              </a>
              <br>
              <strong>Yanghao Li*</strong>, Chao-Yuan Wu*, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, Christoph Feichtenhofer*
              <br>
              <em>arXiv</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2101.02702">Paper</a>

              <!-- <p></p>
              <p>We study Multiscale Vision Transformers (MViTv2) as a unified architecture for image and video classification, as well as object detection.</p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/vit_det.png' alt="game" width="180" style="border-style: none">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2111.11429">
                <papertitle>Benchmarking Detection Transfer Learning with Vision Transformers</papertitle>
              </a>
              <br>
              <strong>Yanghao Li</strong>, Saining Xie, Xinlei Chen, Piotr Dollár, Kaiming He, Ross Girshick
              <br>
              <em>arXiv</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2111.11429">Paper</a>

              <!-- <p></p>
              <p>Benchmarking standard ViT models as the backbone of Mask R-CNN using different ViT initializations. </p> -->
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/mae.png' alt="game" width="180" style="border-style: none">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2111.06377">
                <papertitle>Masked Autoencoders are Scalable Vision Learners</papertitle>
              </a>
              <br>
              Kaiming He, Xinlei Chen, Saining Xie, <strong>Yanghao Li</strong>, Piotr Dollár, Ross Girshick
              <br>
              <em>arXiv</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2111.06377">Paper</a>
              /
              <a href="https://github.com/facebookresearch/mae">Code</a>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/mvit.png' alt="game" width="180" style="border-style: none">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2104.11227">
                <papertitle>Multiscale Vision Transformers</papertitle>
              </a>
              <br>
              Haoqi Fan*, Bo Xiong*, Karttikeya Mangalam*, <strong>Yanghao Li*</strong>, Zhicheng Yan, Jitendra Malik, Christoph Feichtenhofer*
              <br>
              <em>ICCV</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2104.11227">Paper</a>
              /
              <a href="https://github.com/facebookresearch/SlowFast">Code</a>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/ego-exo.png' alt="game" width="180" style="border-style: none">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2104.07905">
                <papertitle>Ego-Exo: Transferring Visual Representations from Third-person to First-person Videos</papertitle>
              </a>
              <br>
              <strong>Yanghao Li</strong>, Tushar Nagarajan, Bo Xiong, Kristen Grauman
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2104.07905">Paper</a>
              /
              <a href="https://github.com/facebookresearch/Ego-Exo">Code</a>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/ego4d.png' alt="game" width="180" style="border-style: none">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2110.07058">
                <papertitle>Ego4d: Around the world in 3,000 hours of egocentric video</papertitle>
              </a>
              <br>
              Kristen Grauman et al.
              <br>
              <em>arXiv</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2110.07058">Paper</a>
              /
              <a href="https://ego4d-data.org/">Website</a>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/pytorchvideo.gif' alt="game" width="180" style="border-style: none">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2111.09887">
                <papertitle>PyTorchVideo: A Deep Learning Library for Video Understanding</papertitle>
              </a>
              <br>
              Haoqi Fan*, Tullie Murrell*, Heng Wang‡, Kalyan Vasudev Alwala‡, <strong>Yanghao Li‡</strong>, Yilei Li‡, Bo Xiong ‡,
              Nikhila Ravi, Meng Li, Haichuan Yang, Jitendra Malik, Ross Girshick, Matt Feiszli, Aaron Adcock†, Wan-Yen Lo†, Christoph Feichtenhofer†
              <br>
              <em>ACM MM</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2111.09887">Paper</a>
              /
              <a href="https://github.com/facebookresearch/pytorchvideo">Code</a>
              /
              <a href="https://pytorchvideo.org/">Website</a>

            </td>
          </tr>

          <!-- <tr><td><h3>2020</h3></td></tr> -->

         

          <tr><td><h3>Previous</h3></td></tr>


          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/ego-topo.png' alt="game" width="180" style="border-style: none">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2001.04583">
                <papertitle>EGO-TOPO: Environment Affordances from Egocentric Video</papertitle>
              </a>
              <br>
              Tushar Nagarajan, <strong>Yanghao Li</strong>, Christoph Feichtenhofer, Kristen Grauman
              <br>
              <em>CVPR</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2001.04583">Paper</a>
              /
              <a href="https://github.com/facebookresearch/ego-topo">Code</a>
              /
              <a href="https://vision.cs.utexas.edu/projects/ego-topo/">Website</a>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/tridentnet.png' alt="game" width="180" style="border-style: none">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1901.01892">
                <papertitle>Scale-Aware Trident Networks for Object Detection</papertitle>
              </a>
              <br>
              <strong>Yanghao Li*</strong>, Yuntao Chen*, Naiyan Wang, Zhaoxiang Zhang
              <br>
              <em>ICCV</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1901.01892">Paper</a>
              /
              <a href="https://github.com/TuSimple/simpledet/tree/master/models/tridentnet">MXNet Code</a>
              /
              <a href="https://github.com/facebookresearch/detectron2/tree/main/projects/TridentNet">Detectron2 Code</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/tbn.png' alt="game" width="180" style="border-style: none">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1811.09974">
                <papertitle>Temporal Bilinear Networks for Video Action Recognition</papertitle>
              </a>
              <br>
              <strong>Yanghao Li</strong>, Sijie Song, Yuqi Li, Jiaying Liu
              <br>
              <em>AAAI</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1811.09974">Paper</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/adabn.png' alt="game" width="180" style="border-style: none">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://www.icst.pku.edu.cn/struct/Pub%20Files/2018/lyh_pr18.pdf">
                <papertitle>Adaptive Batch Normalization for Practical Domain Adaptation</papertitle>
              </a>
              <br>
              <strong>Yanghao Li</strong>, Naiyan Wang, Jianping Shi, Xiaodi Hou, and Jiaying Liu
              <br>
              <em>PR</em>, 2018
              <br>
              <a href="https://www.icst.pku.edu.cn/struct/Pub%20Files/2018/lyh_pr18.pdf">Paper</a>
              /
              <a href="https://openreview.net/pdf?id=Hk6dkJQFx">ICLR workshop</a>
              /
              <a href="https://www.icst.pku.edu.cn/struct/Projects/AdaBN.html">Webpage</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/fbn.png' alt="game" width="180" style="border-style: none">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1611.05709">
                <papertitle>Factorized Bilinear Models for Image Recognition</papertitle>
              </a>
              <br>
              <strong>Yanghao Li</strong>, Naiyan Wang, Jiaying Liu, Xiaodi Hou
              <br>
              <em>ICCV</em>, 2017
              <br>
              <a href="https://arxiv.org/abs/1611.05709">Paper</a>
              /
              <a href="https://github.com/lyttonhao/Factorized-Bilinear-Network">Code</a>
            </td>
          </tr>
        
          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/demy_style.png' alt="game" width="180" style="border-style: none">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1701.01036">
                <papertitle>Demystifying Neural Style Transfer</papertitle>
              </a>
              <br>
              <strong>Yanghao Li</strong>, Naiyan Wang, Jiaying Liu, Xiaodi Hou
              <br>
              <em>ICCV</em>, 2017
              <br>
              <a href="https://arxiv.org/abs/1701.01036">Paper</a>
              /
              <a href="https://github.com/lyttonhao/Neural-Style-MMD">Code</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <img src='images/oad.png' alt="game" width="180" style="border-style: none">
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1604.05633">
                <papertitle>Online Human Action Detection using Joint Classification-Regression Recurrent Neural Networks</papertitle>
              </a>
              <br>
              <strong>Yanghao Li</strong>, Cuiling Lan, Junliang Xing, Wenjun Zeng, Chunfeng Yuan, Jiaying Liu
              <br>
              <em>ECCV</em>, 2016
              <br>
              <a href="https://arxiv.org/abs/1604.05633">Paper</a>
              /
              <a href="https://www.icst.pku.edu.cn/struct/Projects/OAD.html">Website</a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h1>Opensource Projects</h1>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
       
        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/pyslowfast.gif' alt="game" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://github.com/facebookresearch/SlowFast">
              <papertitle>PySlowFast: video understanding codebase for state-of-the-art research</papertitle>
            </a>
            <br>
            Haoqi Fan, <strong>Yanghao Li</strong>, Wan-Yen Lo, Christoph Feichtenhofer
            <br>
            <a href="https://github.com/facebookresearch/SlowFast"><img alt="GitHub stars" src="https://img.shields.io/github/stars/facebookresearch/SlowFast?style=social"></a>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/pytorchvideo.gif' alt="game" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://github.com/facebookresearch/pytorchvideo">
              <papertitle>PyTorchVideo: A Deep Learning Library for Video Understanding</papertitle>
            </a>
            <br>
            Haoqi Fan*, Tullie Murrell*, Heng Wang‡, Kalyan Vasudev Alwala‡, <strong>Yanghao Li‡</strong>, Yilei Li‡, Bo Xiong ‡,
            Nikhila Ravi, Meng Li, Haichuan Yang, Jitendra Malik, Ross Girshick, Matt Feiszli, Aaron Adcock†, Wan-Yen Lo†, Christoph Feichtenhofer†
            <br>
            <a href="https://github.com/facebookresearch/pytorchvideo"><img alt="GitHub stars" src="https://img.shields.io/github/stars/facebookresearch/pytorchvideo?style=social"></a>
            <br>
            <a href="https://arxiv.org/abs/2111.09887?context=cs.LG">Paper</a>
            /
            <a href="https://ai.facebook.com/blog/pytorchvideo-a-deep-learning-library-for-video-understanding/">Post</a>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;vertical-align:middle">
            <img src='images/simpledet.png' alt="game" width="180" style="border-style: none">
          </td>
          <td style="padding:20px;width:70%;vertical-align:middle">
            <a href="https://github.com/TuSimple/simpledet">
              <papertitle>SimpleDet - A Simple and Versatile Framework for Object Detection and Instance Recognition</papertitle>
            </a>
            <br>
            Yuntao Chen, Chenxia Han, <strong>Yanghao Li</strong>, Zehao Huang, Yi Jiang, Naiyan Wang, Zhaoxiang Zhang
            <br>
            <em>JMLR</em>, 2019
            <br>
            <a href="https://github.com/TuSimple/simpledet"><img alt="GitHub stars" src="https://img.shields.io/github/stars/TuSimple/simpledet?style=social"></a>
            <br>
            <a href="https://arxiv.org/abs/1903.05831">Paper</a>
          </td>
        </tr>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Last update: Mar. 2022 &nbsp&nbsp&nbsp&nbsp <a href="https://jonbarron.info/">Template</a>
                </p>
              </td>
            </tr>
          </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
